[{"title":"跟Kaggle Kernel玩数据探索(1) - 可视化","date":"2017-06-27T14:35:25.000Z","path":"2017/06/27/Kaggle_data_visualization/","text":"继上一篇链家爬虫之后，一直还没来的及做数据探索。这么巧这天闲逛Kaggle，发现一个getting started级别的比赛跟房价预测相关，就去kernal参考学习了一下各位大牛的Notebook，在这记录一下学习笔记和心得。 本文基本上是跟随Pedro Marcelino大神的Notebook里的逻辑，他写的非常详尽，而且文风俏皮。他在这个Notebook里重点记载了以下几个步骤： 了解问题 - 看看每一个变量，分析他们的含义和重要性 单变量分析 - 探索因变量SalePrice 多变量分析 - 探索自变量和因变量之间的关系 简单清洗 - 清洗数据，处理缺失值、极值和分类变量 验证假设 - 看看数据集是否满足多元数据分析中的假设 我也跟随这个思路来~ 了解问题 我们先来看一下train data。果然是入门级比赛，除掉SalePrice一共有79个变量，1460行数据，用Excel也可以看到全局。作为第一步，Pedro建议我们做一张spreadsheet（见下文的table）来初步的了解每一个变量，并且标记出直觉上与目标变量的关联性。 虽然只有不到80个变量，但是由于对美国房产一无所知，我基本上是一脸懵逼。比如为什么会有那么多关于面积的变量，LandSlope这到底是个什么鬼，又或者买房的时候谁要管Wood deck area有多大啦！只能先上Google做点小小的research，发现美国的房产交易和国内真的有巨大不同！比如，美国的房产在计算面积的时候，是仅仅考虑地面以上的居住面积的，不包括basement和车库（天哪噜），这个“面积”在题目里面对应的应该是GrLivArea这个变量。又或者他们的Bath卫浴居然是分全卫浴和1/2卫浴（仅有洗手间，不带洗澡设施）的。 最后，我浏览了几个美国的网上房屋中介，发现他们在房源的介绍页面一般都会highlight出来这些属性： 建筑年代、面积、房屋类型、卧室个数、卫浴个数和车库个数。我比较好奇的是建筑年代难道比楼龄更加直观嘛。不过看了一下数据集，鉴于销售年份也仅只有2006-2010这五年间，就暂且不对楼龄进行处理吧。 回到题目里，相关的是这些变量YearBuilt、GrLivArea、BldgType、Bedroom、FullBath、HalfBath、GarageCars，加上我个人比较喜欢的Neighborhood，姑且认为他们与要预测的售价是高度相关的吧。说到底，直觉和research只是帮助分析问题，最终还是要看数据表现。 变量 数据类型 分类 期望 YearBuilt Date Building High GrLivArea Numerical Space High BldgType Categorical Building High Bedroom Numerical Space High FullBath&amp;HalfBath Numerical Space High GarageCars Numerical Space High Neighborhood Categorical Location High 我们先来把总浴室个数加到数据集里： 123import pandas as pddf_train = pd.read_csv('train.csv')df_train['Bath']=df_train['FullBath']+df_train['HalfBath']*0.5 单变量分析先看看因变量SalePrice的基本统计量： 1df_train['SalePrice'].describe() output:123456789count 1460.000000mean 180921.195890std 79442.502883min 34900.00000025% 129975.00000050% 163000.00000075% 214000.000000max 755000.000000Name: SalePrice, dtype: float64 最小值35k，均值181k，最大值755k，应该有个长尾，来画个直方图看看： 1234import matplotlib.pyplot as pltimport seaborn as snssns.distplot(df_train['SalePrice'])plt.show() 直方图用pandas或者matplotlib也可以画，这里用seaborn，多了一条密度曲线。果然跟我们猜测的差不多，SalePrice不是正态分布而是右偏的，可以计算峰度和偏度： 1234df_train['SalePrice'].skew()#1.882876 =&gt; 右偏df_train['SalePrice'].kurt()#6.536282 =&gt; 尖峰 接下来就来看看自变量与因变量之间的关系~ 多变量分析数值变量研究数值变量，画散点图当然是最直观的方法，不过由于变量较多，Pedro建议先用相关性热力图来探索全局。 1234corrmat = df_train.corr()#设置图片大小f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True) 在这张图里，颜色越深，代表相关性越大。我们会注意到一些自变量之间是具有很强的相关性的，如GarageX系列，在特征选择的过程中需要考虑。然后我们截取与SalePrice相关性最大的15个变量单独来看： 123cols = corrmat.abs().nlargest(15, 'SalePrice')['SalePrice'].indextencorr = df_train[cols].corr()sns.heatmap(tencorr,annot=True, square=True) 可以看到在图中是有很多对高度相关的变量的，在线性模型里这会引发多重共线性的问题。当然，如果使用Lasso回归等方法就可以规避，所以我们暂且不对变量进行筛选，只是画图的时候不重复。接下来就可以画散点图了，seaborn有很棒的pairplot功能，可以一次呈现多个变量。 123sns.set()cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'Bath','YearBuilt','MasVnrArea','Fireplaces']sns.pairplot(df_train[cols], size = 2.5) 这张图上有些小发现： 有较明显的离群点；outlier对线性模型的影响很大，应考虑去除（详情见下一篇） 我们并不知道OverallQual是怎么评价的，但是这个变量除了和SalePrice呈现非常强的相关性之外，与两个面积变量GrLivArea和TotalBsmtSF也有紧密的联系 建筑年代越近的房子，TotalBsmtSF也越大 散点图对数值型变量非常的有用，对分类变量则可以用箱线图。 分类变量对分类变量的处理一直都比较tricky，比如说ordinal变量是否量化成数值变量，或者如何减少单个变量里的类型。画图探索也是比较重要的一个步骤。 这是我们比较关心的Neighborhood变量，可以看出的是不同类别下的价格的确是不同的，而且某些类别的分布比较接近，应该可以合并。另外在字典表中可以看到地点Iowa State University，按照我的理解，在预测的时候，可以加上当地的一些宏观经济指标。 123456var = 'Neighborhood'data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(16, 8))fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)fig.axis(ymin=0, ymax=800000)plt.xticks(rotation=90) 有的指标可以带上直方图一起参考，如建筑类型，就被1Fam主导了。 1234fig, ax = plt.subplots(1, 2, figsize = (10, 4))sns.boxplot('BldgType', 'SalePrice', data = df_train, ax = ax[0]).set(ylim = (0, 400000))sns.countplot('BldgType', data = df_train)plt.tight_layout() 还可以画在一张图里。诶，供暖和AC都有比较明显的区分度诶？ 12345fig, ax = plt.subplots(2, 2, figsize = (10, 8))sns.boxplot('Heating', 'SalePrice', data = df_train, ax = ax[0, 0])sns.boxplot('HeatingQC', 'SalePrice', data = df_train, ax = ax[0, 1])sns.boxplot('CentralAir', 'SalePrice', data = df_train, ax = ax[1, 0])sns.boxplot('Electrical', 'SalePrice', data = df_train, ax = ax[1, 1]) 其他还有许多变量就不一一列举了。先给自己挖个坑，下期写数据预处理~","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"数据可视化","slug":"数据可视化","permalink":"http://yoursite.com/tags/数据可视化/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"}]},{"title":"Python3.6之爬虫初体验--深圳房价猎奇","date":"2017-06-07T07:54:22.000Z","path":"2017/06/07/Hi-Amigo/","text":"写在前面话说有一天南瓜小姐收到一个关于住宅地址的需求，跟香蕉先生讨论了一番之后，觉得不妨先收集一些地址下来探索一番。 作为学前班级别的程序员（大雾），正巧想熟悉一下Python3的套路，便萌生了爬取链家的想法。学习了几个晚上写好了程序，跑了两天，爬下来几万条数据，觉得还蛮好玩的，在此温故而知新一下。 页面探索起了爬虫这个小念头之后，还有很多困惑要先解决。从哪个页面开始爬？怎么跳转？在哪里循环？还是先探索一下链家的网页吧！ 打开链家深圳小区板块的主页，这是一个索引页面，每页有30个小区的简单介绍，点击小区图片可跳转。虽然一开始只是想收集地址信息，不过既然都爬起来了，就索性再怀抱猎奇的心态瞅瞅其他的信息（房价）吧。 一个有意思的地方是，页面上显示： 共找到 4784 个小区 可是在页面的底端，只有100个页码签，如果按1-100跳转，只能最多获取到3000个小区的信息，应该是链家限制了展示的上限。 不过没关系，再观察发现，在条件过滤区域，有一个“区域”选择标签，下面有十个深圳的行政区选项。如果在选择不同的行政区之后再进行跳转，由于没有一个行政区的小区数量大于3000，就可以不漏掉任何一个小区了。平常跟巨大数据量打交道的多，也是万万没想到，深圳逆天的房价只base在几千个楼盘上。 在网站上溜达了几圈之后，我大致梳理了一个爬取的流程—— 通过不同行政区下的小区索引，获得小区主页面链接 在小区主页面，获得小区基本信息、成交和在售页面链接 通过成交和在售链接获取相应的信息 想清楚了这些，就来撸起袖子写code~ Urllib是个啥东东我们得到了目标url之后，要怎么访问和读取他呢？这里必须要提到urllib模块，因为他真的——特别简单实用！当我要访问某个网站的时候，我就这样写： 123import urllib.requesturl = \"http://sz.lianjia.com/xiaoqu/nanshanqu\"print(urllib.request.urlopen(url).read().decode()) 小白如我，只凭三行代码就获取到了整个网页，Python大法好！ 在Python2.7里，是大名鼎鼎urllib2这个模块负责跟网页交互。到了Python3的版本，urllib2变成了urllib.request，具体的内容和变动建议查看官方文档。 回到我们的爬虫，加上循环读取一定量（大约100）的网页之后，链家作为一个有一定规模的企业，不出意外也有他的反爬措施——提示我的IP流量异常。 作为应对，先加个随机浏览器header试试： 12345678hds = [&#123;'User-Agent':\"Mozilla/5.0 (Windows NT 5.1; rv:37.0) Gecko/20100101 Firefox/37.0\"&#125;,\\ &#123;'User_Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0\"&#125;,\\ &#123;'User_Agent':\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; GTB7.0)\"&#125;,\\ &#123;'User_Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1\"&#125;,\\ &#123;'User_Agent':\"Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50\"&#125;]req = urllib.request.Request(url, headers=random.choice(hds))urlop = urllib.request.urlopen(req) 伪装浏览器头让我们一次可以访问的页面多了一些，但是对于成千上万的量级还是不够，看来需要在IP上费点心。 在度娘上可以找到许多使用免费代理的攻略，不过我试了一下，也不知道是不是打开方式不对，很不稳定，索性就去买了一个收费的版本，非常好用。加上代理IP的代码最后会变成这样子： 12345678910111213141516171819#proxy详情来自代理提供商proxy_handler = urllib.request.ProxyHandler(&#123;'http': proxy&#125;)opener = urllib.request.build_opener(proxy_handler)urllib.request.install_opener(opener)#循环读取链接url = 'http://sz.lianjia.com/xiaoqu/'districts = ['luohuqu', 'futianqu', 'nanshanqu', 'yantianqu', 'baoanqu', 'longgangqu', 'longhuaqu', 'guangmingxinqu', 'pingshanqu', 'dapengxinqu']for district in districts: urld = url+district #下文再讲获取total_pages for i in range(total_pages): url_dist = urld +'/pg' + str(i+1) try: req = urllib.request.Request(url, headers=random.choice(hds)) urlop = urllib.request.urlopen(req) html = urlop.read().decode() print('访问成功!') except Exception as e: print(e) 好，现在可以读取网页了，但是要肿么从一堆HTML结构里找到我们要的信息咧？下一步就到解析了~ BeautifulSoup是个啥东东BeautifulSoup是Python提供的解析HTML和XML的强大工具，可以轻松的用id/class名称或其他组件来定位想要的内容。 举个栗子，上文代码中提到获取页面数量，在原始的HTML中，是这样的：1&lt;div class=\"page-box house-lst-page-box\" comp-module='page' page-url=\"/xiaoqu/nanshanqu/pg&#123;page&#125;/\"page-data='&#123;\"totalPage\":31,\"curPage\":1&#125;'&gt;&lt;/div&gt; 用BeautifulSoup解析，代码是这样的： 1234567891011from bs4 import BeautifulSoupimport json#将上文获得的html变成经过解析的soupsoup = BeautifulSoup(html, \"html.parser\")def get_total_page(soup): #通过class的名字获取内容 div = soup.find('div', &#123;'class': 'page-box house-lst-page-box'&#125;).get('page-data') #page-data得到的是一个字符串，用json库将他变成json串 d = json.loads(div) total_pages = d['totalPage'] return total_pages 再举栗子，在小区链接的索引页面，每一个小区主页面链接是在一个个组件里：12&lt;li class=\"clear xiaoquListItem\"&gt; &lt;a class=\"img\" href=\"https://sz.lianjia.com/xiaoqu/2411049487692/\" target=\"_blank\" rel=\"nofollow\"&gt; 可以使用find_all函数读取所有class名字相同的组件： 12for x in soup.find_all(class_=\"clear xiaoquListItem\"): link = x.a['href'] 同理，在小区详情页面，可以读取出相应的小区名称、地址、单价等等： 1234567title = soup.find(class_=\"detailTitle\").get_text()address = soup.find(class_=\"detailDesc\").get_text()unit_price_div = soup.find(class_=\"xiaoquUnitPrice\")if unit_price_div is not None: unit_price = unit_price_div.get_text()else: unit_price = \"none\" 获取成交、在售等信息的原理是一样的。 我在爬链家的时候，仅仅用了少许这个库的功能，如果想了解更多，官方文档戳这里。 数据库连接最后一个重要的步骤就是将读取出来的信息保存到数据库，我用的是mysql。区别于以前的MySQLdb，Python3有一个全新的连接包pymysql，虽然在用法上没感觉有什么不同。 连接数据库的代码示例如下： 123456import pymysqlconn = pymysql.connect(host='localhost', port=3306, user='root', passwd='1234', db='lianjia', charset='UTF8')c = conn.cursor()sql_line = \"insert into \" + tablename + \" values (%s, %s, %s)\"c.execute(sql_line, record)conn.commit() 当然噜，mysql里的表是需要事先建好哒。 我将小区、成交、在售信息分别存成了三张表，最后得到了四千多个小区、五万多条历史成交、两万多正在挂牌的信息。虽然第一次写爬虫代码，不会多线程，封装的也不好，但也是一次宝贵的体验。 最后，出于好奇，我看了看挂牌价格最高（五月底）的几套房源： 感觉干活更有劲了呢，科科~","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]}]