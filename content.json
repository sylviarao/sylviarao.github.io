[{"title":"从五万多条深圳房产交易信息里，我看到了什么有趣的事情","date":"2017-07-24T03:13:04.000Z","path":"2017/07/24/Shenzhen_housing_price_visualization/","text":"写在前面继上次在链家爬取深圳住宅小区、成交记录和二手房在售信息之后，终于有机会来瞅瞅这些数据长什么样子。作为一个可视化小白，我简单的用pandas和matplotlib里的画图工具做了一些简陋的图（有多简陋，可以参考我的另一篇博文，是浏览kaggle kernel时候写的读书笔记，那里面的图美极了）。在美学上没什么天赋的我，每到画图的时候都无比想念那些可爱的BI 工具。 小区信息我的数据在mysql上，先用pandas的函数读取出来。 12345678# encoding:UTF-8import pandas as pdimport numpy as npimport sqlalchemy as SAimport matplotlib.pyplot as pltengine = SA.create_engine('mysql+pymysql://username:password@localhost/database?charset=utf8')xiaoqu = pd.read_sql_table('xiaoqu', engine) 在这个表中，存在一些’none’或’暂无信息’的字段，我们替换成空值便于计算。 12xiaoqu=xiaoqu.replace('none',np.NaN)xiaoqu=xiaoqu.replace('暂无信息',np.NaN) 地址信息处理 爬取下来的地址信息是长这个样子的: 由于只做简单的区域性统计，就只把括号里的大区和小区截取出来变成两个新的字段neighborhood和district。 12xiaoqu['neighborhood']=xiaoqu['address'].str.replace(\"(\",\"\").str.split(\")\").str[0]xiaoqu['district']=xiaoqu['neighborhood'].str.replace(\"区\",\"区/\").str.split('/').str[0] 地址信息的统计先来看看每个district（行政区）的小区数量： 1234district_count=pd.DataFrame(xiaoqu.groupby('district') .agg(&#123;'title':np.size,'unitprice':np.mean&#125;) .rename(columns=&#123;'title':'count','unitprice':'avg_unitprice'&#125;))district_count['count'].plot(kind='barh') 跟想象中一样，罗湖福田南山这三个区域的楼盘是最多的，而大鹏、坪山、光明新区则分别只有几十个楼盘。 单价分布接下来可以看看楼盘单价的分布，在链家网上单价指的是上一个月的二手房参考均价，比如我是五月份爬取的信息，所以这里的单价实际上是四月份该小区二手房的参考均价。他们的直方图大概是这样： 12xiaoqu['unitprice']=pd.to_numeric(xiaoqu['unitprice'])xiaoqu.plot.hist(stacked=True, bins=20) 这是一个明显带长尾的右偏图，也符合日常的认知。在分区域的箱线图中我们也看到了很多极值。 1xiaoqu[['district','unitprice']].boxplot(by='district') 精确到每个区域的neighborhood来看，离群点（尤其单价高于20w）几乎都来自南山贵到飞起来的传统豪宅区华侨城、红树湾和蛇口。还有一点好玩的是，南山白石洲片区的楼盘都集中在低价区域，但是随着这几个月旧改的落实，应该很快就要成为历史了。 建筑年份和单价的关系也可以用散点图看一下 12xiaoqu['builtyear']=pd.to_numeric(xiaoqu['builtdate'].str.replace(\"年建成\",\"\"))xiaoqu[['builtyear','unitprice']].plot.scatter(x='builtyear',y='unitprice') 噢，居然还有70年代的小区仍然在流动？ 总的来讲，越新的小区越有可能卖的更贵，不过21世纪以后建的好像都差不太远。 接下来我们看下成交记录。 成交信息成交信息需要清洗的部分比较多，我们先清洗一下。 12345678910111213141516171819202122deal_raw=pd.read_sql_table('chengjiao', engine)deal_raw=pd.read_sql_table('chengjiao', engine)deal_raw=deal_raw.replace('none',np.NaN)deal_raw=deal_raw.replace('未知',np.NaN)deal_raw=deal_raw.replace('暂无数据',np.NaN)deal_raw=deal_raw.replace('暂无数据挂牌价格（万）',np.NaN)deal_raw=deal_raw.replace('暂无数据成交周期（天）',np.NaN)deal_raw=deal_raw.replace('暂无数据浏览（次）',np.NaN)deal_raw['total_price']=pd.to_numeric(deal_raw['成交价格'].str.split('万').str[0])deal_raw['unit_price']=pd.to_numeric(deal_raw['成交价格'].str.split('万').str[1]. str.replace('元/平',''))deal_raw['listed_price']=pd.to_numeric(deal_raw['挂牌价格'].str.split('挂牌').str[0])deal_raw['deal_days']=pd.to_numeric(deal_raw['成交周期'].str.split('成交').str[0])deal_raw['price_adj']=pd.to_numeric(deal_raw['调价'].str.split('调价').str[0])deal_raw['daikan']=pd.to_numeric(deal_raw['带看'].str.split('带看').str[0])deal_raw['guanzhu']=pd.to_numeric(deal_raw['关注'].str.split('关注').str[0])deal_raw['liulan']=pd.to_numeric(deal_raw['浏览'].str.split('浏览').str[0])deal_raw['floor_location']=deal_raw['所在楼层'].str.split('(').str[0]deal_raw['sqr_ft']=pd.to_numeric(deal_raw['建筑面积'].str.split('㎡').str[0])deal_raw['deal_date']=pd.to_datetime(deal_raw['成交时间'])deal_raw['deal_month']=pd.to_datetime(deal_raw['deal_date'].apply(lambda x: x.strftime('%B-%Y')))deal_raw['apartment_type']=deal_raw['房屋户型'].str.slice(0,4) 这个表格里没有成交小区的位置信息，所以要join一下之前小区的那个表得到更完全的信息（列明中英交错有点挫。。。）。 12345deal_joint=pd.merge(deal_raw,xiaoqu,how='left',left_on='小区名称',right_on='title')[['小区名称', 'deal_date','deal_month','total_price','unit_price', 'listed_price','deal_days','price_adj','daikan','guanzhu', 'liulan','floor_location','apartment_type','sqr_ft','房屋朝向','建成年代', '装修情况','梯户比例','配备电梯','unitprice','district','neighborhood']] 时间序列在时间序列上，我们来看看成交量以及平均单价的趋势。 1234567891011fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(12,9))time_price_district=pd.pivot_table(deal_joint,'unit_price','deal_month','district',aggfunc=np.mean)time_deal_count=deal_joint[['deal_month','unit_price']].groupby('deal_month').count()time_price_district.plot(ax=axes[0])axes[0].set_title('区域均价')axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=5)axes[0].get_xaxis().set_visible(False)time_deal_count.plot(ax=axes[1],legend=False)axes[1].set_title('成交量')axes[1].set_xlabel('成交时间') 可以看到链家仅仅公布了10年以后成交的信息，10-15年是一个缓慢爬升的状态，15年开始则打了鸡血似的攀升，然后16年开始是比较稳定的震荡。对每个区来讲，趋势都是比较接近的（坪山和光明新区建区晚，楼盘少，且不看）。 挂牌价格链家公布的挂牌价格（总价）里，有一些极端的值，所以看挂牌价格的时候先把大于一亿的点去掉，再画散点图。 1deal_joint[deal_joint['listed_price']&lt;10000].plot.scatter(x='listed_price',y='total_price') 这个散点图告诉我们，挂牌价格到成交价格基本上有一成左右的议价空间，低于两千万的交易议价空间更大。 房屋属性我也很想知道，除了外部属性（如宏观经济、小区所处位置、是否学区房等）之外，房子本身的属性是否对价格有比较显著的影响呢。 比如说楼层。在这里我先把时间线拉近到16年以后，因为从时间序列上看，16年以后虽然价格仍然在波动，但是在波动范围上比较稳定。 12deal_joint_recent=deal_joint[deal_joint['deal_date']&gt;pd.to_datetime('2016-01-01')]deal_joint_recent[['floor_location','unit_price']].boxplot(by='floor_location') 不知道这里的地下室指的是什么，不过高、中、低楼层在成交均价上似乎没有什么显著的差异。 在户型上，大户型均价范围相对更大一些，但小户型（尤其是1室的）也不乏极端高位的单价。 按面积来看，似乎也是这样。 装修情况上，除了毛坯的相对低一点，其他的似乎对成交单价的影响也不大。 其他的指标也没有显示出明显的影响因素，所以果然还是外部因素对房价的影响比较大么（摊手）。当然噜，链家本身列举出来的房屋属性就不多。哪怕是在地址上，也没有清洗出诸如交通便利程度、学位、配套等等的量化指标。 这个故事告诉我们，不要心存侥幸，还是好好搬砖掌握好timing才能在自己喜爱的neighborhood买到心仪的房产。祝大伙儿心想事成~","tags":[{"name":"数据可视化","slug":"数据可视化","permalink":"http://yoursite.com/tags/数据可视化/"},{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"},{"name":"matplotlib","slug":"matplotlib","permalink":"http://yoursite.com/tags/matplotlib/"},{"name":"深圳","slug":"深圳","permalink":"http://yoursite.com/tags/深圳/"},{"name":"housing","slug":"housing","permalink":"http://yoursite.com/tags/housing/"}]},{"title":"跟Kaggle Kernel玩数据探索(1) - 可视化","date":"2017-06-27T14:35:25.000Z","path":"2017/06/27/Kaggle_data_visualization/","text":"继上一篇链家爬虫之后，一直还没来的及做数据探索。这么巧这天闲逛Kaggle，发现一个getting started级别的比赛跟房价预测相关，就去kernal参考学习了一下各位大牛的Notebook，在这记录一下学习笔记和心得。 本文基本上是跟随Pedro Marcelino大神的Notebook里的逻辑，他写的非常详尽，而且文风俏皮。他在这个Notebook里重点记载了以下几个步骤： 了解问题 - 看看每一个变量，分析他们的含义和重要性 单变量分析 - 探索因变量SalePrice 多变量分析 - 探索自变量和因变量之间的关系 简单清洗 - 清洗数据，处理缺失值、极值和分类变量 验证假设 - 看看数据集是否满足多元数据分析中的假设 我也跟随这个思路来~ 了解问题 我们先来看一下train data。果然是入门级比赛，除掉SalePrice一共有79个变量，1460行数据，用Excel也可以看到全局。作为第一步，Pedro建议我们做一张spreadsheet（见下文的table）来初步的了解每一个变量，并且标记出直觉上与目标变量的关联性。 虽然只有不到80个变量，但是由于对美国房产一无所知，我基本上是一脸懵逼。比如为什么会有那么多关于面积的变量，LandSlope这到底是个什么鬼，又或者买房的时候谁要管Wood deck area有多大啦！只能先上Google做点小小的research，发现美国的房产交易和国内真的有巨大不同！比如，美国的房产在计算面积的时候，是仅仅考虑地面以上的居住面积的，不包括basement和车库（天哪噜），这个“面积”在题目里面对应的应该是GrLivArea这个变量。又或者他们的Bath卫浴居然是分全卫浴和1/2卫浴（仅有洗手间，不带洗澡设施）的。 最后，我浏览了几个美国的网上房屋中介，发现他们在房源的介绍页面一般都会highlight出来这些属性： 建筑年代、面积、房屋类型、卧室个数、卫浴个数和车库个数。我比较好奇的是建筑年代难道比楼龄更加直观嘛。不过看了一下数据集，鉴于销售年份也仅只有2006-2010这五年间，就暂且不对楼龄进行处理吧。 回到题目里，相关的是这些变量YearBuilt、GrLivArea、BldgType、Bedroom、FullBath、HalfBath、GarageCars，加上我个人比较喜欢的Neighborhood，姑且认为他们与要预测的售价是高度相关的吧。说到底，直觉和research只是帮助分析问题，最终还是要看数据表现。 变量 数据类型 分类 期望 YearBuilt Date Building High GrLivArea Numerical Space High BldgType Categorical Building High Bedroom Numerical Space High FullBath&amp;HalfBath Numerical Space High GarageCars Numerical Space High Neighborhood Categorical Location High 我们先来把总浴室个数加到数据集里： 123import pandas as pddf_train = pd.read_csv('train.csv')df_train['Bath']=df_train['FullBath']+df_train['HalfBath']*0.5 单变量分析先看看因变量SalePrice的基本统计量： 1df_train['SalePrice'].describe() output:123456789count 1460.000000mean 180921.195890std 79442.502883min 34900.00000025% 129975.00000050% 163000.00000075% 214000.000000max 755000.000000Name: SalePrice, dtype: float64 最小值35k，均值181k，最大值755k，应该有个长尾，来画个直方图看看： 1234import matplotlib.pyplot as pltimport seaborn as snssns.distplot(df_train['SalePrice'])plt.show() 直方图用pandas或者matplotlib也可以画，这里用seaborn，多了一条密度曲线。果然跟我们猜测的差不多，SalePrice不是正态分布而是右偏的，可以计算峰度和偏度： 1234df_train['SalePrice'].skew()#1.882876 =&gt; 右偏df_train['SalePrice'].kurt()#6.536282 =&gt; 尖峰 接下来就来看看自变量与因变量之间的关系~ 多变量分析数值变量研究数值变量，画散点图当然是最直观的方法，不过由于变量较多，Pedro建议先用相关性热力图来探索全局。 1234corrmat = df_train.corr()#设置图片大小f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True) 在这张图里，颜色越深，代表相关性越大。我们会注意到一些自变量之间是具有很强的相关性的，如GarageX系列，在特征选择的过程中需要考虑。然后我们截取与SalePrice相关性最大的15个变量单独来看： 123cols = corrmat.abs().nlargest(15, 'SalePrice')['SalePrice'].indextencorr = df_train[cols].corr()sns.heatmap(tencorr,annot=True, square=True) 可以看到在图中是有很多对高度相关的变量的，在线性模型里这会引发多重共线性的问题。当然，如果使用Lasso回归等方法就可以规避，所以我们暂且不对变量进行筛选，只是画图的时候不重复。接下来就可以画散点图了，seaborn有很棒的pairplot功能，可以一次呈现多个变量。 123sns.set()cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'Bath','YearBuilt','MasVnrArea','Fireplaces']sns.pairplot(df_train[cols], size = 2.5) 这张图上有些小发现： 有较明显的离群点；outlier对线性模型的影响很大，应考虑去除（详情见下一篇） 我们并不知道OverallQual是怎么评价的，但是这个变量除了和SalePrice呈现非常强的相关性之外，与两个面积变量GrLivArea和TotalBsmtSF也有紧密的联系 建筑年代越近的房子，TotalBsmtSF也越大 散点图对数值型变量非常的有用，对分类变量则可以用箱线图。 分类变量对分类变量的处理一直都比较tricky，比如说ordinal变量是否量化成数值变量，或者如何减少单个变量里的类型。画图探索也是比较重要的一个步骤。 这是我们比较关心的Neighborhood变量，可以看出的是不同类别下的价格的确是不同的，而且某些类别的分布比较接近，应该可以合并。另外在字典表中可以看到地点Iowa State University，按照我的理解，在预测的时候，可以加上当地的一些宏观经济指标。 123456var = 'Neighborhood'data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(16, 8))fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)fig.axis(ymin=0, ymax=800000)plt.xticks(rotation=90) 有的指标可以带上直方图一起参考，如建筑类型，就被1Fam主导了。 1234fig, ax = plt.subplots(1, 2, figsize = (10, 4))sns.boxplot('BldgType', 'SalePrice', data = df_train, ax = ax[0]).set(ylim = (0, 400000))sns.countplot('BldgType', data = df_train)plt.tight_layout() 还可以画在一张图里。诶，供暖和AC都有比较明显的区分度诶？ 12345fig, ax = plt.subplots(2, 2, figsize = (10, 8))sns.boxplot('Heating', 'SalePrice', data = df_train, ax = ax[0, 0])sns.boxplot('HeatingQC', 'SalePrice', data = df_train, ax = ax[0, 1])sns.boxplot('CentralAir', 'SalePrice', data = df_train, ax = ax[1, 0])sns.boxplot('Electrical', 'SalePrice', data = df_train, ax = ax[1, 1]) 其他还有许多变量就不一一列举了。先给自己挖个坑，下期写数据预处理~","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"数据可视化","slug":"数据可视化","permalink":"http://yoursite.com/tags/数据可视化/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"}]},{"title":"Python3.6之爬虫初体验--深圳房价猎奇","date":"2017-06-07T07:54:22.000Z","path":"2017/06/07/Hi-Amigo/","text":"写在前面话说有一天南瓜小姐收到一个关于住宅地址的需求，跟香蕉先生讨论了一番之后，觉得不妨先收集一些地址下来探索一番。 作为学前班级别的程序员（大雾），正巧想熟悉一下Python3的套路，便萌生了爬取链家的想法。学习了几个晚上写好了程序，跑了两天，爬下来几万条数据，觉得还蛮好玩的，在此温故而知新一下。 页面探索起了爬虫这个小念头之后，还有很多困惑要先解决。从哪个页面开始爬？怎么跳转？在哪里循环？还是先探索一下链家的网页吧！ 打开链家深圳小区板块的主页，这是一个索引页面，每页有30个小区的简单介绍，点击小区图片可跳转。虽然一开始只是想收集地址信息，不过既然都爬起来了，就索性再怀抱猎奇的心态瞅瞅其他的信息（房价）吧。 一个有意思的地方是，页面上显示： 共找到 4784 个小区 可是在页面的底端，只有100个页码签，如果按1-100跳转，只能最多获取到3000个小区的信息，应该是链家限制了展示的上限。 不过没关系，再观察发现，在条件过滤区域，有一个“区域”选择标签，下面有十个深圳的行政区选项。如果在选择不同的行政区之后再进行跳转，由于没有一个行政区的小区数量大于3000，就可以不漏掉任何一个小区了。平常跟巨大数据量打交道的多，也是万万没想到，深圳逆天的房价只base在几千个楼盘上。 在网站上溜达了几圈之后，我大致梳理了一个爬取的流程—— 通过不同行政区下的小区索引，获得小区主页面链接 在小区主页面，获得小区基本信息、成交和在售页面链接 通过成交和在售链接获取相应的信息 想清楚了这些，就来撸起袖子写code~ Urllib是个啥东东我们得到了目标url之后，要怎么访问和读取他呢？这里必须要提到urllib模块，因为他真的——特别简单实用！当我要访问某个网站的时候，我就这样写： 123import urllib.requesturl = \"http://sz.lianjia.com/xiaoqu/nanshanqu\"print(urllib.request.urlopen(url).read().decode()) 小白如我，只凭三行代码就获取到了整个网页，Python大法好！ 在Python2.7里，是大名鼎鼎urllib2这个模块负责跟网页交互。到了Python3的版本，urllib2变成了urllib.request，具体的内容和变动建议查看官方文档。 回到我们的爬虫，加上循环读取一定量（大约100）的网页之后，链家作为一个有一定规模的企业，不出意外也有他的反爬措施——提示我的IP流量异常。 作为应对，先加个随机浏览器header试试： 12345678hds = [&#123;'User-Agent':\"Mozilla/5.0 (Windows NT 5.1; rv:37.0) Gecko/20100101 Firefox/37.0\"&#125;,\\ &#123;'User_Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0\"&#125;,\\ &#123;'User_Agent':\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; GTB7.0)\"&#125;,\\ &#123;'User_Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1\"&#125;,\\ &#123;'User_Agent':\"Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50\"&#125;]req = urllib.request.Request(url, headers=random.choice(hds))urlop = urllib.request.urlopen(req) 伪装浏览器头让我们一次可以访问的页面多了一些，但是对于成千上万的量级还是不够，看来需要在IP上费点心。 在度娘上可以找到许多使用免费代理的攻略，不过我试了一下，也不知道是不是打开方式不对，很不稳定，索性就去买了一个收费的版本，非常好用。加上代理IP的代码最后会变成这样子： 12345678910111213141516171819#proxy详情来自代理提供商proxy_handler = urllib.request.ProxyHandler(&#123;'http': proxy&#125;)opener = urllib.request.build_opener(proxy_handler)urllib.request.install_opener(opener)#循环读取链接url = 'http://sz.lianjia.com/xiaoqu/'districts = ['luohuqu', 'futianqu', 'nanshanqu', 'yantianqu', 'baoanqu', 'longgangqu', 'longhuaqu', 'guangmingxinqu', 'pingshanqu', 'dapengxinqu']for district in districts: urld = url+district #下文再讲获取total_pages for i in range(total_pages): url_dist = urld +'/pg' + str(i+1) try: req = urllib.request.Request(url, headers=random.choice(hds)) urlop = urllib.request.urlopen(req) html = urlop.read().decode() print('访问成功!') except Exception as e: print(e) 好，现在可以读取网页了，但是要肿么从一堆HTML结构里找到我们要的信息咧？下一步就到解析了~ BeautifulSoup是个啥东东BeautifulSoup是Python提供的解析HTML和XML的强大工具，可以轻松的用id/class名称或其他组件来定位想要的内容。 举个栗子，上文代码中提到获取页面数量，在原始的HTML中，是这样的：1&lt;div class=\"page-box house-lst-page-box\" comp-module='page' page-url=\"/xiaoqu/nanshanqu/pg&#123;page&#125;/\"page-data='&#123;\"totalPage\":31,\"curPage\":1&#125;'&gt;&lt;/div&gt; 用BeautifulSoup解析，代码是这样的： 1234567891011from bs4 import BeautifulSoupimport json#将上文获得的html变成经过解析的soupsoup = BeautifulSoup(html, \"html.parser\")def get_total_page(soup): #通过class的名字获取内容 div = soup.find('div', &#123;'class': 'page-box house-lst-page-box'&#125;).get('page-data') #page-data得到的是一个字符串，用json库将他变成json串 d = json.loads(div) total_pages = d['totalPage'] return total_pages 再举栗子，在小区链接的索引页面，每一个小区主页面链接是在一个个组件里：12&lt;li class=\"clear xiaoquListItem\"&gt; &lt;a class=\"img\" href=\"https://sz.lianjia.com/xiaoqu/2411049487692/\" target=\"_blank\" rel=\"nofollow\"&gt; 可以使用find_all函数读取所有class名字相同的组件： 12for x in soup.find_all(class_=\"clear xiaoquListItem\"): link = x.a['href'] 同理，在小区详情页面，可以读取出相应的小区名称、地址、单价等等： 1234567title = soup.find(class_=\"detailTitle\").get_text()address = soup.find(class_=\"detailDesc\").get_text()unit_price_div = soup.find(class_=\"xiaoquUnitPrice\")if unit_price_div is not None: unit_price = unit_price_div.get_text()else: unit_price = \"none\" 获取成交、在售等信息的原理是一样的。 我在爬链家的时候，仅仅用了少许这个库的功能，如果想了解更多，官方文档戳这里。 数据库连接最后一个重要的步骤就是将读取出来的信息保存到数据库，我用的是mysql。区别于以前的MySQLdb，Python3有一个全新的连接包pymysql，虽然在用法上没感觉有什么不同。 连接数据库的代码示例如下： 123456import pymysqlconn = pymysql.connect(host='localhost', port=3306, user='root', passwd='1234', db='lianjia', charset='UTF8')c = conn.cursor()sql_line = \"insert into \" + tablename + \" values (%s, %s, %s)\"c.execute(sql_line, record)conn.commit() 当然噜，mysql里的表是需要事先建好哒。 我将小区、成交、在售信息分别存成了三张表，最后得到了四千多个小区、五万多条历史成交、两万多正在挂牌的信息。虽然第一次写爬虫代码，不会多线程，封装的也不好，但也是一次宝贵的体验。 最后，出于好奇，我看了看挂牌价格最高（五月底）的几套房源： 感觉干活更有劲了呢，科科~","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]}]