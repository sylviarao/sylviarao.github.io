[{"title":"Python3.6之爬虫初体验--深圳房价猎奇","date":"2017-06-07T07:54:22.000Z","path":"2017/06/07/Hi-Amigo/","text":"写在前面话说有一天南瓜小姐收到一个关于住宅地址的需求，跟香蕉先生讨论了一番之后，觉得不妨先收集一些地址下来探索一番。 作为学前班级别的程序员（大雾），正巧想熟悉一下Python3的套路，便萌生了爬取链家的想法。学习了几个晚上写好了程序，跑了两天，爬下来几万条数据，觉得还蛮好玩的，在此温故而知新一下。 页面探索起了爬虫这个小念头之后，还有很多困惑要先解决。从哪个页面开始爬？怎么跳转？在哪里循环？还是先探索一下链家的网页吧！ 打开链家深圳小区板块的主页，这是一个索引页面，每页有30个小区的简单介绍，点击小区图片可跳转。虽然一开始只是想收集地址信息，不过既然都爬起来了，就索性再怀抱猎奇的心态瞅瞅其他的信息（房价）吧。 一个有意思的地方是，页面上显示： 共找到 4784 个小区 可是在页面的底端，只有100个页码签，如果按1-100跳转，只能最多获取到3000个小区的信息，应该是链家限制了展示的上限。 不过没关系，再观察发现，在条件过滤区域，有一个“区域”选择标签，下面有十个深圳的行政区选项。如果在选择不同的行政区之后再进行跳转，由于没有一个行政区的小区数量大于3000，就可以不漏掉任何一个小区了。平常跟巨大数据量打交道的多，也是万万没想到，深圳逆天的房价只base在几千个楼盘上。 在网站上溜达了几圈之后，我大致梳理了一个爬取的流程—— 通过不同行政区下的小区索引，获得小区主页面链接 在小区主页面，获得小区基本信息、成交和在售页面链接 通过成交和在售链接获取相应的信息 想清楚了这些，就来撸起袖子写code~ Urllib是个啥东东我们得到了目标url之后，要怎么访问和读取他呢？这里必须要提到urllib模块，因为他真的——特别简单实用！当我要访问某个网站的时候，我就这样写： 123import urllib.requesturl = \"http://sz.lianjia.com/xiaoqu/nanshanqu\"print(urllib.request.urlopen(url).read().decode()) 小白如我，只凭三行代码就获取到了整个网页，Python大法好！ 在Python2.7里，是大名鼎鼎urllib2这个模块负责跟网页交互。到了Python3的版本，urllib2变成了urllib.request，具体的内容和变动建议查看官方文档。 回到我们的爬虫，加上循环读取一定量（大约100）的网页之后，链家作为一个有一定规模的企业，不出意外也有他的反爬措施——提示我的IP流量异常。 作为应对，先加个随机浏览器header试试： 12345678hds = [&#123;'User-Agent':\"Mozilla/5.0 (Windows NT 5.1; rv:37.0) Gecko/20100101 Firefox/37.0\"&#125;,\\ &#123;'User_Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0\"&#125;,\\ &#123;'User_Agent':\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; GTB7.0)\"&#125;,\\ &#123;'User_Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1\"&#125;,\\ &#123;'User_Agent':\"Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50\"&#125;]req = urllib.request.Request(url, headers=random.choice(hds))urlop = urllib.request.urlopen(req) 伪装浏览器头让我们一次可以访问的页面多了一些，但是对于成千上万的量级还是不够，看来需要在IP上费点心。 在度娘上可以找到许多使用免费代理的攻略，不过我试了一下，也不知道是不是打开方式不对，很不稳定，索性就去买了一个收费的版本，非常好用。加上代理IP的代码最后会变成这样子： 12345678910111213141516171819#proxy详情来自代理提供商proxy_handler = urllib.request.ProxyHandler(&#123;'http': proxy&#125;)opener = urllib.request.build_opener(proxy_handler)urllib.request.install_opener(opener)#循环读取链接url = 'http://sz.lianjia.com/xiaoqu/'districts = ['luohuqu', 'futianqu', 'nanshanqu', 'yantianqu', 'baoanqu', 'longgangqu', 'longhuaqu', 'guangmingxinqu', 'pingshanqu', 'dapengxinqu']for district in districts: urld = url+district #下文再讲获取total_pages for i in range(total_pages): url_dist = urld +'/pg' + str(i+1) try: req = urllib.request.Request(url, headers=random.choice(hds)) urlop = urllib.request.urlopen(req) html = urlop.read().decode() print('访问成功!') except Exception as e: print(e) 好，现在可以读取网页了，但是要肿么从一堆HTML结构里找到我们要的信息咧？下一步就到解析了~ BeautifulSoup是个啥东东BeautifulSoup是Python提供的解析HTML和XML的强大工具，可以轻松的用id/class名称或其他组件来定位想要的内容。 举个栗子，上文代码中提到获取页面数量，在原始的HTML中，是这样的：1&lt;div class=\"page-box house-lst-page-box\" comp-module='page' page-url=\"/xiaoqu/nanshanqu/pg&#123;page&#125;/\"page-data='&#123;\"totalPage\":31,\"curPage\":1&#125;'&gt;&lt;/div&gt; 用BeautifulSoup解析，代码是这样的： 1234567891011from bs4 import BeautifulSoupimport json#将上文获得的html变成经过解析的soupsoup = BeautifulSoup(html, \"html.parser\")def get_total_page(soup): #通过class的名字获取内容 div = soup.find('div', &#123;'class': 'page-box house-lst-page-box'&#125;).get('page-data') #page-data得到的是一个字符串，用json库将他变成json串 d = json.loads(div) total_pages = d['totalPage'] return total_pages 再举栗子，在小区链接的索引页面，每一个小区主页面链接是在一个个组件里：12&lt;li class=\"clear xiaoquListItem\"&gt; &lt;a class=\"img\" href=\"https://sz.lianjia.com/xiaoqu/2411049487692/\" target=\"_blank\" rel=\"nofollow\"&gt; 可以使用find_all函数读取所有class名字相同的组件： 12for x in soup.find_all(class_=\"clear xiaoquListItem\"): link = x.a['href'] 同理，在小区详情页面，可以读取出相应的小区名称、地址、单价等等： 1234567title = soup.find(class_=\"detailTitle\").get_text()address = soup.find(class_=\"detailDesc\").get_text()unit_price_div = soup.find(class_=\"xiaoquUnitPrice\")if unit_price_div is not None: unit_price = unit_price_div.get_text()else: unit_price = \"none\" 获取成交、在售等信息的原理是一样的。 我在爬链家的时候，仅仅用了少许这个库的功能，如果想了解更多，官方文档戳这里。 数据库连接最后一个重要的步骤就是将读取出来的信息保存到数据库，我用的是mysql。区别于以前的MySQLdb，Python3有一个全新的连接包pymysql，虽然在用法上没感觉有什么不同。 连接数据库的代码示例如下： 123456import pymysqlconn = pymysql.connect(host='localhost', port=3306, user='root', passwd='1234', db='lianjia', charset='UTF8')c = conn.cursor()sql_line = \"insert into \" + tablename + \" values (%s, %s, %s)\"c.execute(sql_line, record)conn.commit() 当然噜，mysql里的表是需要事先建好哒。 我将小区、成交、在售信息分别存成了三张表，最后得到了四千多个小区、五万多条历史成交、两万多正在挂牌的信息。虽然第一次写爬虫代码，不会多线程，封装的也不好，但也是一次宝贵的体验。 最后，出于好奇，我看了看挂牌价格最高（五月底）的几套房源： 感觉干活更有劲了呢，科科~","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]}]